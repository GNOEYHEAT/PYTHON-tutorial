{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Vectors : 어휘 사전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"good\"의 synonym(유의어)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n': 'noun', 'v': 'verb', 's': 'adj (s)', 'a': 'adj', 'r': 'adv'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses = {'n':'noun', 'v':'verb', 's':'adj (s)', 'a':'adj', 'r':'adv'}\n",
    "poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun   : good\n",
      "noun   : good, goodness\n",
      "noun   : good, goodness\n",
      "noun   : commodity, trade_good, good\n",
      "adj    : good\n",
      "adj (s): full, good\n",
      "adj    : good\n",
      "adj (s): estimable, good, honorable, respectable\n",
      "adj (s): beneficial, good\n",
      "adj (s): good\n",
      "adj (s): good, just, upright\n",
      "adj (s): adept, expert, good, practiced, proficient, skillful, skilful\n",
      "adj (s): good\n",
      "adj (s): dear, good, near\n",
      "adj (s): dependable, good, safe, secure\n",
      "adj (s): good, right, ripe\n",
      "adj (s): good, well\n",
      "adj (s): effective, good, in_effect, in_force\n",
      "adj (s): good\n",
      "adj (s): good, serious\n",
      "adj (s): good, sound\n",
      "adj (s): good, salutary\n",
      "adj (s): good, honest\n",
      "adj (s): good, undecomposed, unspoiled, unspoilt\n",
      "adj (s): good\n",
      "adv    : well, good\n",
      "adv    : thoroughly, soundly, good\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets(\"good\"):\n",
    "    pos = poses[synset.pos()] # POS Tag, Part-Of-Speech: 품사\n",
    "    lemmas = [lemma.name() for lemma in synset.lemmas()] # Lemma : 단어의 원형\n",
    "    lemmas = ', '.join(lemmas)\n",
    "    print(f\"{pos:7s}: {lemmas}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"cat\"의 hypernym(상위어)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('feline.n.01'),\n",
       " Synset('carnivore.n.01'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('mammal.n.01'),\n",
       " Synset('vertebrate.n.01'),\n",
       " Synset('chordate.n.01'),\n",
       " Synset('animal.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('entity.n.01')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = wn.synset(\"cat.n.01\")\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(cat.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어휘 사전의 문제점 : 단어와 단어 간의 유사도(similartiy)를 계산할 수 없음!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVD Method Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[0, 2, 2, 0, 0, 0, 0, 0, 0, 0], # I\n",
    "              [2, 0, 0, 1, 0, 0, 0, 0, 0, 0], # ate\n",
    "              [2, 0, 0, 0, 0, 0, 1, 0, 1, 0], # like\n",
    "              [0, 1, 0, 0, 1, 1, 0, 0, 0, 0], # a\n",
    "              [0, 0, 0, 1, 0, 0, 0, 0, 0, 1], # banana\n",
    "              [0, 0, 0, 1, 0, 0, 0, 0, 0, 1], # cherry\n",
    "              [0, 0, 1, 0, 0, 0, 0, 1, 0, 0], # deep\n",
    "              [0, 0, 0, 0, 0, 0, 1, 0, 0, 1], # learning\n",
    "              [0, 0, 1, 0, 0, 0, 0, 0, 0, 1], # NLP\n",
    "              [0, 0, 0, 0, 1, 1, 0, 1, 1, 0]]) # ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_svd\n",
    "U, S, V_t = np.linalg.svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta : 0.8174988844487285\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "theta = sum(S[:k]) / sum(S)\n",
    "print(f'theta : {theta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.58951321,  0.63338858, -0.07381218, -0.31561873,  0.08921915],\n",
       "       [-0.43880494, -0.4679372 , -0.0950592 , -0.15540716,  0.40200628],\n",
       "       [-0.49906814, -0.51894471,  0.18296896, -0.18902671, -0.32887242],\n",
       "       [-0.21718772,  0.19140752,  0.37405398,  0.29204662,  0.48061906],\n",
       "       [-0.12612756, -0.06426321, -0.39796596,  0.39641327,  0.19296524],\n",
       "       [-0.12612756, -0.06426321, -0.39796596,  0.39641327,  0.19296524],\n",
       "       [-0.19421905,  0.18666537,  0.02954001,  0.04266901, -0.41679693],\n",
       "       [-0.11890892, -0.06274144, -0.25333285,  0.28215586, -0.35443303],\n",
       "       [-0.21471749,  0.16369162, -0.31774506,  0.17599977, -0.30080158],\n",
       "       [-0.184132  ,  0.00884932,  0.57389513,  0.57316285, -0.16426787]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U = U[:, :k]\n",
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  2.,  2., -0.,  0.,  0., -0.,  0., -0.,  0.],\n",
       "       [ 2.,  0., -0.,  1., -0., -0.,  0., -0.,  0.,  0.],\n",
       "       [ 2., -0.,  0.,  0.,  0.,  0.,  1.,  0.,  1., -0.],\n",
       "       [-0.,  1.,  0.,  0.,  1.,  1., -0.,  0.,  0., -0.],\n",
       "       [ 0., -0.,  0.,  1.,  0.,  0.,  0., -0., -0.,  1.],\n",
       "       [ 0., -0.,  0.,  1.,  0.,  0.,  0., -0., -0.,  1.],\n",
       "       [-0.,  0.,  1., -0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0., -0.,  0.,  0., -0., -0.,  0.,  0.,  0.,  1.],\n",
       "       [-0.,  0.,  1.,  0., -0., -0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0., -0., -0.,  1.,  1.,  0.,  1.,  1.,  0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "US = np.dot(U, np.diag(S[:k]))\n",
    "X_hat = np.dot(US, V_t[:k ,:]) # shape\n",
    "X_hat = np.round(X_hat)\n",
    "X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "bow = ['I', 'ate', 'like', 'a',\n",
    "      'banana', 'cherry', 'deep', 'learning', 'NLP', '.']\n",
    "\n",
    "fig = plt.figure(1, figsize=(8, 5))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "for idx, word in enumerate(bow):\n",
    "    ax.axis([-.75, .75, -.75, .75])\n",
    "    ax.text3D(U[idx, 0], U[idx, 1], U[idx, 2], word, size=15)\n",
    "    \n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SVD Method의 문제점\n",
    "    * 새로운 단어 출현으로 인해 Matrix의 차원(dimension)이 자주 바뀜 -> SVD 다시 계산\n",
    "    * 대부분의 단어가 동시에 출현(co-occur)하지 않으므로 Matrix가 매우 스파스(sparse) 함\n",
    "    * 일반적으로 Matrix는 매우 높은 차원을 가짐\n",
    "    * 계산 비용이 높음 -> mxn Matrix인 경우 O(mn^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SVD Method의 문제점 보완 방법들\n",
    "    * 'the', 'he', 'has' 등 불용어(stop words) 제거\n",
    "    * Window size에서 단어의 거리를 반영해주는 ramp window 적용\n",
    "    * 단순 카운팅이 아닌 Pearson correlation & negative count 사용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Sampling (NEG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'The quick brwon fox jumps over the lazy dog'\n",
    "text = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {idx: word for idx, word in enumerate(text)}\n",
    "word2index = {word: idx for idx, word in index2word.items()}\n",
    "seq = [word2index[word] for word in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(over, lazy), 1\n",
      "(fox, over), 1\n",
      "(the, lazy), 1\n",
      "(fox, fox), 0\n",
      "(fox, quick), 1\n",
      "(quick, fox), 0\n",
      "(over, jumps), 0\n",
      "(jumps, brwon), 1\n",
      "(jumps, over), 1\n",
      "(fox, brwon), 1\n",
      "(the, brwon), 0\n",
      "(dog, lazy), 0\n",
      "(over, fox), 1\n",
      "(jumps, the), 1\n",
      "(brwon, fox), 1\n",
      "(brwon, quick), 1\n",
      "(lazy, dog), 1\n",
      "(the, jumps), 1\n",
      "(lazy, over), 1\n",
      "(jumps, dog), 0\n",
      "(lazy, the), 1\n",
      "(the, quick), 0\n",
      "(brwon, dog), 0\n",
      "(quick, fox), 1\n",
      "(over, the), 1\n",
      "(fox, jumps), 1\n",
      "(the, lazy), 0\n",
      "(the, dog), 1\n",
      "(the, over), 1\n",
      "(quick, brwon), 1\n",
      "(brwon, fox), 0\n",
      "(dog, the), 1\n",
      "(over, fox), 0\n",
      "(jumps, fox), 1\n",
      "(dog, lazy), 1\n",
      "(brwon, quick), 0\n",
      "(over, jumps), 1\n",
      "(dog, brwon), 0\n",
      "(brwon, jumps), 1\n"
     ]
    }
   ],
   "source": [
    "pairs, labels = skipgrams(sequence=seq, vocabulary_size=len(word2index.keys()),\n",
    "                         window_size=2, negative_samples=.5)\n",
    "\n",
    "for pair, label in zip(pairs, labels):\n",
    "    w_i, w_o = pair\n",
    "    print(f'({index2word[w_i]}, {index2word[w_o]}), {label}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
